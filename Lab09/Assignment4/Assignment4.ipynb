{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imports and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Device Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    augmentation_train = transforms.Compose([\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomAffine(degrees=20, translate=(0.1, 0.1), scale=(0.85, 1.15)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    augmentation_test = transforms.Compose([transforms.ToTensor()])\n",
    "    return augmentation_train, augmentation_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, mode=\"train\", transform=None):\n",
    "        print(f\"\\nLoading MNIST {mode} dataset...\")\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.data = torchvision.datasets.MNIST(\n",
    "            root=\"./data\", train=(mode == \"train\"), download=True\n",
    "        )\n",
    "        print(f\"Total length: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.data[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        image = image.view(-1) \n",
    "        label = F.one_hot(torch.tensor(label), num_classes=10).float()\n",
    "        return {\"index\": index, \"image\": image, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. DataLoader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_transform, test_transform, batch_size, num_workers):\n",
    "    train_dataset = MNISTDataset(mode=\"train\", transform=train_transform)\n",
    "    test_dataset = MNISTDataset(mode=\"test\", transform=test_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "batch_size, num_workers = 100, 0\n",
    "train_transform, test_transform = get_transforms()\n",
    "train_loader, test_loader = get_dataloaders(train_transform, test_transform, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Neural Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, sizes, loss_fn):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            in_dim, out_dim = sizes[i]\n",
    "            layers.extend([nn.Linear(in_dim, out_dim), nn.BatchNorm1d(out_dim), nn.ReLU()])\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(sizes[-1][0], sizes[-1][1])\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        if y is not None:\n",
    "            loss = self.loss_fn(x, y)\n",
    "            return loss, x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "network = NeuralNetwork(sizes=[[784, 1024], [1024, 1024], [1024, 1024], [1024, 10]], loss_fn=loss_function)\n",
    "network = network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(net, dataloader, optimizer, scheduler, device):\n",
    "    network.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", unit=\"batch\"):\n",
    "        images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss, outputs = net(images, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    train_accuracy = 100. * train_correct / len(dataloader.dataset)\n",
    "    return train_loss / len(dataloader.dataset), train_accuracy\n",
    "\n",
    "def evaluate(net, dataloader, loss_fn, device):\n",
    "    network.eval()\n",
    "    test_loss, test_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Testing\", unit=\"batch\"):\n",
    "            images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            outputs = net(images)\n",
    "            test_loss += loss_fn(outputs, labels).item()\n",
    "            test_correct += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "    test_accuracy = 100. * test_correct / len(dataloader.dataset)\n",
    "    return test_loss / len(dataloader.dataset), test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate, weight_decay, momentum = 0.1, 1e-6, 0.9\n",
    "epochs, milestones, gamma = 100, [25, 50, 75], 0.1\n",
    "\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "best_accuracy, best_epoch = -1, -1\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    train_loss, train_acc = train_one_epoch(network, train_loader, optimizer, scheduler, device)\n",
    "    test_loss, test_acc = evaluate(network, test_loader, loss_function, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.2f}% | Test Loss: {test_loss:.6f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    if test_acc > best_accuracy:\n",
    "        best_accuracy = test_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(network.state_dict(), \"best_model.pt\")\n",
    "\n",
    "print(f\"\\nBest Test Accuracy: {best_accuracy:.2f}% at epoch {best_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(network, dataloader, device, output_file=\"submission.csv\"):\n",
    "    print(\"\\nLoading the best model for submission...\")\n",
    "    network.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    network.eval()\n",
    "\n",
    "    submission_data = {\"ID\": [], \"target\": []}\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating Submission\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            ids = batch[\"index\"].cpu().numpy()\n",
    "            outputs = network(images)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            submission_data[\"ID\"].extend(ids)\n",
    "            submission_data[\"target\"].extend(predictions)\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df.sort_values(by=\"ID\", inplace=True)\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission file '{output_file}' generated.\")\n",
    "\n",
    "generate_submission(network, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
